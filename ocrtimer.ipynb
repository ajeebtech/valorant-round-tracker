{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "# Valorant Round Timer Extraction with TensorFlow Lite Model Maker\n",
        "\n",
        "This notebook trains a TensorFlow Lite model to extract round timer values from Valorant screenshots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "## Overview\n",
        "\n",
        "This notebook will:\n",
        "1. Train an image classification model to recognize timer values from Valorant screenshots\n",
        "2. Export the model as TensorFlow Lite for on-device inference\n",
        "3. The model can classify cropped timer regions into specific time values (e.g., \"1:14\", \"0:45\", etc.)\n",
        "\n",
        "## Dataset Structure\n",
        "\n",
        "Your training data should be organized as follows:\n",
        "```\n",
        "data/\n",
        "  train/\n",
        "    1_14/     # Images showing timer at 1:14\n",
        "      image1.jpg\n",
        "      image2.jpg\n",
        "      ...\n",
        "    1_13/     # Images showing timer at 1:13\n",
        "      image1.jpg\n",
        "      ...\n",
        "    0_45/     # Images showing timer at 0:45\n",
        "      ...\n",
        "  test/\n",
        "    1_14/\n",
        "      ...\n",
        "    1_13/\n",
        "      ...\n",
        "```\n",
        "\n",
        "**Note:** Use underscores instead of colons in folder names (e.g., `1_14` instead of `1:14`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw5Y7snSuG51"
      },
      "source": [
        "### Install Required Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr3q-gvm3cI8"
      },
      "source": [
        "Install TensorFlow Lite Model Maker and required dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLF2PKkSbV3"
      },
      "source": [
        "### Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvAObmTqglq"
      },
      "source": [
        "Import TensorFlow and Model Maker libraries for image classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhl8lqVamEty"
      },
      "outputs": [],
      "source": [
        "# Install TensorFlow Lite Model Maker\n",
        "%pip install -q tflite-model-maker\n",
        "%pip install -q pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6lRhVK9Q_0U"
      },
      "source": [
        "Now let's import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import image_classifier\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker.image_classifier import DataLoader\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Crop Timer Regions from Screenshots\n",
        "\n",
        "Before training, you need to crop the timer regions from your full Valorant screenshots.\n",
        "\n",
        "Use the `crop_timer.py` script or the functions below to automatically detect and crop timer regions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install OpenCV for image processing\n",
        "%pip install -q opencv-python matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the timer cropper\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "from crop_timer_notebook import crop_timer_region, batch_crop_timers, TimerCropper\n",
        "\n",
        "print(\"✓ Timer cropper loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test on a Single Screenshot\n",
        "\n",
        "Test the cropping on a single screenshot to verify it works correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test cropping on screenshot.png\n",
        "test_image = 'screenshot.png'\n",
        "\n",
        "if os.path.exists(test_image):\n",
        "    print(f\"Testing crop timer on: {test_image}\")\n",
        "    # Crop and show preview\n",
        "    cropped = crop_timer_region(\n",
        "        test_image, \n",
        "        show_preview=True,  # Shows original with box + cropped result\n",
        "        method='heuristic'  # Uses fixed position (timer is always in same place)\n",
        "    )\n",
        "    if cropped is not None:\n",
        "        print(f\"✓ Cropped image shape: {cropped.shape}\")\n",
        "        print(f\"✓ Successfully cropped timer region!\")\n",
        "    else:\n",
        "        print(\"✗ Failed to crop timer region\")\n",
        "else:\n",
        "    print(f\"⚠️  Test image not found: {test_image}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Crop All Screenshots\n",
        "\n",
        "Crop timer regions from all screenshots in a directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch crop all screenshots\n",
        "input_screenshots_dir = 'screenshots'  # Folder with your full screenshots\n",
        "output_cropped_dir = 'cropped_timers'  # Output folder for cropped images\n",
        "\n",
        "if os.path.exists(input_screenshots_dir):\n",
        "    # Count screenshots\n",
        "    image_files = [f for f in os.listdir(input_screenshots_dir) \n",
        "                   if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "    \n",
        "    if len(image_files) > 0:\n",
        "        print(f\"Found {len(image_files)} screenshot(s) to process...\")\n",
        "        success_count = batch_crop_timers(\n",
        "            input_screenshots_dir,\n",
        "            output_cropped_dir,\n",
        "            method='heuristic'  # Uses fixed position (fastest and most reliable)\n",
        "        )\n",
        "        print(f\"\\n✓ Cropped {success_count} timer regions\")\n",
        "        print(f\"  Output folder: {output_cropped_dir}\")\n",
        "        print(f\"  Next: Organize cropped images into data/train/ and data/test/ folders by time value\")\n",
        "    else:\n",
        "        print(f\"⚠️  No screenshots found in {input_screenshots_dir}\")\n",
        "        print(\"   Add your Valorant screenshots to the 'screenshots/' folder and run this cell again\")\n",
        "else:\n",
        "    print(f\"⚠️  Screenshots directory not found: {input_screenshots_dir}\")\n",
        "    print(\"   The 'screenshots/' folder has been created - add your Valorant screenshots there\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adjust Detection Parameters (if needed)\n",
        "\n",
        "If the automatic detection doesn't work well for your screenshots, you can adjust the detection parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a custom cropper with adjusted parameters\n",
        "from crop_timer_notebook import TimerCropper\n",
        "import cv2\n",
        "\n",
        "# Adjust output size if needed (width, height)\n",
        "custom_cropper = TimerCropper(output_size=(250, 120))  # Larger output\n",
        "\n",
        "# Test with custom settings\n",
        "test_image = 'screenshots/screenshot1.jpg'  # Update this path\n",
        "\n",
        "if os.path.exists(test_image):\n",
        "    cropped = custom_cropper.crop_timer(\n",
        "        test_image,\n",
        "        method='heuristic',  # Use 'heuristic' for consistent positioning\n",
        "        show_preview=True\n",
        "    )\n",
        "else:\n",
        "    print(\"⚠️  Test image not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRd13bfetO7B"
      },
      "source": [
        "## Load Your Training Data\n",
        "\n",
        "**Important:** The model trains on **cropped timer regions**, not full screenshots!\n",
        "\n",
        "**Workflow:**\n",
        "1. Use the cropping functions above (Step 0) to extract timer regions from full screenshots\n",
        "2. Organize the cropped timer images into folders by time value\n",
        "3. Train the model on these cropped images\n",
        "\n",
        "**Data structure:**\n",
        "- `data/train/` - Training images organized by time value folders\n",
        "- `data/test/` - Test images organized by time value folders\n",
        "\n",
        "Each folder should contain cropped timer screenshots showing that specific time value (e.g., `1_14/`, `0_45/`).\n",
        "\n",
        "**Current crop settings:** 12% width, 6% height, output 400×200px (optimized zoom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2BSkxWg6Rhx"
      },
      "outputs": [],
      "source": [
        "# Step 1: Crop screenshots (if you have full screenshots)\n",
        "# Skip this if you already have cropped timer images organized in folders\n",
        "\n",
        "SCREENSHOTS_DIR = 'screenshots'  # Folder with full Valorant screenshots\n",
        "CROPPED_DIR = 'cropped_timers'   # Output folder for cropped timer regions\n",
        "\n",
        "# Set to True if you need to crop screenshots first\n",
        "NEED_TO_CROP = False  # Change to True if you have screenshots to crop\n",
        "\n",
        "if NEED_TO_CROP:\n",
        "    if os.path.exists(SCREENSHOTS_DIR):\n",
        "        print(\"Step 1: Cropping timer regions from screenshots...\")\n",
        "        print(f\"  Input: {SCREENSHOTS_DIR}\")\n",
        "        print(f\"  Output: {CROPPED_DIR}\")\n",
        "        success_count = batch_crop_timers(SCREENSHOTS_DIR, CROPPED_DIR, method='heuristic')\n",
        "        print(f\"\\n✓ Cropped {success_count} timer regions\")\n",
        "        print(f\"  Next: Organize these cropped images into data/train/ and data/test/ folders\")\n",
        "    else:\n",
        "        print(f\"⚠️  Screenshots directory not found: {SCREENSHOTS_DIR}\")\n",
        "        print(\"   Set NEED_TO_CROP = False if you already have cropped images organized\")\n",
        "else:\n",
        "    print(\"✓ Skipping crop step (assuming you already have cropped timer images)\")\n",
        "\n",
        "# Step 2: Set your data directory path\n",
        "# This should point to folders containing cropped timer images organized by time value\n",
        "DATA_DIR = 'data'  # Change this to your actual data path\n",
        "\n",
        "# Check if data directory exists\n",
        "print(f\"\\nStep 2: Checking training data directory...\")\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f\"⚠️  Data directory '{DATA_DIR}' not found!\")\n",
        "    print(\"\\nPlease organize your cropped timer images as follows:\")\n",
        "    print(f\"  {DATA_DIR}/train/1_14/    # Images showing timer at 1:14\")\n",
        "    print(f\"  {DATA_DIR}/train/1_13/    # Images showing timer at 1:13\")\n",
        "    print(f\"  {DATA_DIR}/train/0_45/    # Images showing timer at 0:45\")\n",
        "    print(f\"  ...\")\n",
        "    print(f\"  {DATA_DIR}/test/1_14/     # Test images for 1:14\")\n",
        "    print(f\"  ...\")\n",
        "    print(\"\\nTip: Use batch_crop_timers() above to crop your screenshots first!\")\n",
        "else:\n",
        "    print(f\"✓ Found data directory: {DATA_DIR}\")\n",
        "    # List available classes\n",
        "    train_dir = os.path.join(DATA_DIR, 'train')\n",
        "    if os.path.exists(train_dir):\n",
        "        classes = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n",
        "        print(f\"✓ Found {len(classes)} time classes: {classes[:10]}{'...' if len(classes) > 10 else ''}\")\n",
        "        print(f\"  These are the timer values your model will learn to classify\")\n",
        "    else:\n",
        "        print(f\"⚠️  Training directory not found: {train_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPYTbGrizcTC"
      },
      "source": [
        "## Quickstart: Train Your Timer Classification Model\n",
        "\n",
        "Follow these steps to train your model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLNaOXnl3JQB"
      },
      "outputs": [],
      "source": [
        "# Step 1: Choose a model architecture\n",
        "# We'll use EfficientNet-Lite0 which is optimized for mobile devices\n",
        "spec = model_spec.get('efficientnet_lite0')\n",
        "\n",
        "print(f\"✓ Using model: {spec.name}\")\n",
        "print(f\"  Input image size: {spec.input_image_shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xushUyZXqP59"
      },
      "source": [
        "## Quickstart\n",
        "\n",
        "There are five steps to train a text classification model:\n",
        "\n",
        "**Step 1. Choose a text classification model architecture.**\n",
        "\n",
        "Here we use the average word embedding model architecture, which will produce a small and fast model with decent accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtdZ-JDwMimd"
      },
      "outputs": [],
      "source": [
        "# Load training and test data\n",
        "train_data = DataLoader.from_folder(\n",
        "    os.path.join(DATA_DIR, 'train'),\n",
        "    model_spec=spec\n",
        ")\n",
        "\n",
        "test_data = DataLoader.from_folder(\n",
        "    os.path.join(DATA_DIR, 'test'),\n",
        "    model_spec=spec\n",
        ")\n",
        "\n",
        "print(f\"✓ Loaded {len(train_data)} training images\")\n",
        "print(f\"✓ Loaded {len(test_data)} test images\")\n",
        "print(f\"✓ Number of classes: {len(train_data.index_to_label)}\")\n",
        "print(f\"  Classes: {list(train_data.index_to_label.values())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yug6gR9qyHui"
      },
      "source": [
        "**Step 3: Train the model**\n",
        "\n",
        "Train the image classification model on your timer screenshots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5U-A3tw6Y27"
      },
      "source": [
        "**Step 4: Evaluate the model**\n",
        "\n",
        "Test the model's accuracy on your test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD5BvzWe6YKa"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "# Adjust epochs based on your dataset size and desired accuracy\n",
        "# More epochs = better accuracy but longer training time\n",
        "model = image_classifier.create(\n",
        "    train_data,\n",
        "    model_spec=spec,\n",
        "    epochs=10,  # Start with 10, increase if needed\n",
        "    batch_size=32,\n",
        "    validation_data=test_data\n",
        ")\n",
        "\n",
        "print(\"✓ Model training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uZkLR6N6gDR"
      },
      "source": [
        "**Step 5: Export as TensorFlow Lite model**\n",
        "\n",
        "Export the trained model in TensorFlow Lite format for on-device inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwlYdTcg63xy"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_data)\n",
        "print(f\"✓ Test accuracy: {accuracy:.2%}\")\n",
        "print(f\"✓ Test loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BzCHLWJ6h7q"
      },
      "source": [
        "## Export the Model\n",
        "\n",
        "Export the trained model as a TensorFlow Lite file for deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xmnl6Yy7ARn"
      },
      "outputs": [],
      "source": [
        "# Export the model\n",
        "export_dir = 'valorant_timer_model'\n",
        "model.export(export_dir=export_dir)\n",
        "\n",
        "print(f\"✓ Model exported to: {export_dir}/model.tflite\")\n",
        "print(f\"✓ Model size: {os.path.getsize(os.path.join(export_dir, 'model.tflite')) / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgCDMe0e6jlT"
      },
      "source": [
        "## Test the Model\n",
        "\n",
        "Test the exported model on a sample image to verify it works correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm_UULdW7A9T"
      },
      "outputs": [],
      "source": [
        "# Test the model on a sample image\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load a test image (replace with path to your test image)\n",
        "test_image_path = None  # Set this to a test image path\n",
        "\n",
        "if test_image_path and os.path.exists(test_image_path):\n",
        "    # Load and display the image\n",
        "    img = Image.open(test_image_path)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Test Image')\n",
        "    plt.show()\n",
        "    \n",
        "    # Run inference\n",
        "    result = model.evaluate_tflite(os.path.join(export_dir, 'model.tflite'), test_data)\n",
        "    print(f\"✓ TFLite model accuracy: {result:.2%}\")\n",
        "else:\n",
        "    print(\"⚠️  Set test_image_path to test the model on a specific image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVxaf3x_7OfB"
      },
      "source": [
        "You can download the TensorFlow Lite model file using the left sidebar of Colab. Go into the `average_word_vec` folder as we specified in `export_dir` parameter above, right-click on the `model.tflite` file and choose `Download` to download it to your local computer.\n",
        "\n",
        "This model can be integrated into an Android or an iOS app using the [NLClassifier API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/nl_classifier) of the [TensorFlow Lite Task Library](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview).\n",
        "\n",
        "See the [TFLite Text Classification sample app](https://github.com/tensorflow/examples/blob/master/lite/examples/text_classification/android/lib_task_api/src/main/java/org/tensorflow/lite/examples/textclassification/client/TextClassificationClient.java#L54) for more details on how the model is used in a working app.\n",
        "\n",
        "*Note 1: Android Studio Model Binding does not support text classification yet so please use the TensorFlow Lite Task Library.*\n",
        "\n",
        "*Note 2: There is a `model.json` file in the same folder with the TFLite model. It contains the JSON representation of the [metadata](https://www.tensorflow.org/lite/models/convert/metadata) bundled inside the TensorFlow Lite model. Model metadata helps the TFLite Task Library know what the model does and how to pre-process/post-process data for the model. You don't need to download the `model.json` file as it is only for informational purpose and its content is already inside the TFLite file.*\n",
        "\n",
        "*Note 3: If you train a text classification model using MobileBERT or BERT-Base architecture, you will need to use [BertNLClassifier API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_nl_classifier) instead to integrate the trained model into a mobile app.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l65ctmtW7_FF"
      },
      "source": [
        "The following sections walk through the example step by step to show more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izO7NU7unYot"
      },
      "source": [
        "**Step 6: Use `TFLite Task Library` to demo how to use the trained models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDov6P4wppHO"
      },
      "source": [
        "Read the dev.csv file into the sentence data to predict with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWwvHmIltQC2"
      },
      "outputs": [],
      "source": [
        "sentence_data = pd.read_csv('/content/dev.csv', index_col=0)\n",
        "sentence_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_-bejm5vRBf"
      },
      "source": [
        "Model config parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAEEs3_3vPz5"
      },
      "outputs": [],
      "source": [
        "# Name of the TFLite text classification model.\n",
        "_MODEL = '/content/average_word_vec/model.tflite'\n",
        "# Whether to run the model on EdgeTPU.\n",
        "_ENABLE_EDGETPU = False\n",
        "# Number of CPU threads to run the model.\n",
        "_NUM_THREADS = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bInGjRcOtQbn"
      },
      "source": [
        "Initialize model\n",
        "\n",
        "We could also change the parameters like `file_name`, `use_coral`, and `num_threads` that could affect the model results. The parameters you can adjust are:\n",
        "\n",
        "*   `file_name`: Name of the TFLite image classification model.\n",
        "*   `use_coral`: If true, inference will be delegated to a connected Coral Edge TPU device.\n",
        "*   `num_threads`: Number of CPU threads to run the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Haham4qT8hmV"
      },
      "outputs": [],
      "source": [
        "# Initialize the text classification model.\n",
        "base_options = core.BaseOptions(file_name=_MODEL, use_coral=_ENABLE_EDGETPU, num_threads=_NUM_THREADS)\n",
        "options = text.NLClassifierOptions(base_options)\n",
        "\n",
        "# Create NLClassifier from options.\n",
        "classifier = text.NLClassifier.create_from_options(options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HLl9LC9oA3G"
      },
      "source": [
        "Predict using `TFLite Task Library`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAQDHFs5tTxZ"
      },
      "outputs": [],
      "source": [
        "for idx in range(20):\n",
        "  sentence = sentence_data['sentence'].iloc[idx]\n",
        "  label = sentence_data['label'].iloc[idx]\n",
        "  text_classification_result = classifier.classify(sentence)\n",
        "  classification_list = text_classification_result.classifications[0].categories\n",
        "\n",
        "  # Sort output by probability descending.\n",
        "  predict_label = sorted(\n",
        "      classification_list, key=lambda item: item.score, reverse=True)[0]\n",
        "\n",
        "  print('truth_label: {} -----> predict_label: {}'.format(label, predict_label.category_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ_B8fMDOhMR"
      },
      "source": [
        "## Choose a model architecture for Text Classifier\n",
        "\n",
        "Each `model_spec` object represents a specific model for the text classifier. TensorFlow Lite Model Maker currently supports [MobileBERT](https://arxiv.org/pdf/2004.02984.pdf), averaging word embeddings and [BERT-Base](https://arxiv.org/pdf/1810.04805.pdf) models.\n",
        "\n",
        "| Supported Model          | Name of model_spec      | Model Description                                                                                                     | Model size                                  |\n",
        "|--------------------------|-------------------------|-----------------------------------------------------------------------------------------------------------------------|---------------------------------------------|\n",
        "| Averaging Word Embedding | 'average_word_vec'      | Averaging text word embeddings with RELU activation.                                                                  |           <1MB                             |\n",
        "| MobileBERT               | 'mobilebert_classifier' | 4.3x smaller and 5.5x faster than BERT-Base while achieving competitive results, suitable for on-device applications. | 25MB w/ quantization <br/> 100MB w/o quantization                                        |\n",
        "| BERT-Base                | 'bert_classifier'       | Standard BERT model that is widely used in NLP tasks.                                                                 | 300MB |\n",
        "\n",
        "In the quick start, we have used the average word embedding model. Let's switch to [MobileBERT](https://arxiv.org/pdf/2004.02984.pdf) to train a model with higher accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEAWuZQ1PFiX"
      },
      "outputs": [],
      "source": [
        "mb_spec = model_spec.get('mobilebert_classifier')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygEncJxtl-nQ"
      },
      "source": [
        "## Load training data\n",
        "\n",
        "You can upload your own dataset to work through this tutorial. Upload your dataset by using the left sidebar in Colab.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/download.tensorflow.org/models/tflite/screenshots/model_maker_text_classification.png\" alt=\"Upload File\" width=\"800\" hspace=\"100\">\n",
        "\n",
        "If you prefer not to upload your dataset to the cloud, you can also locally run the library by following the [guide](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWAusqz-WD5i"
      },
      "source": [
        "To keep it simple, we will reuse the SST-2 dataset downloaded earlier. Let's use the `DataLoader.from_csv` method to load the data.\n",
        "\n",
        "Please be noted that as we have changed the model architecture, we will need to reload the training and test dataset to apply the new preprocessing logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_fOlZsklmlL"
      },
      "outputs": [],
      "source": [
        "train_data = DataLoader.from_csv(\n",
        "      filename='train.csv',\n",
        "      text_column='sentence',\n",
        "      label_column='label',\n",
        "      model_spec=mb_spec,\n",
        "      is_training=True)\n",
        "test_data = DataLoader.from_csv(\n",
        "      filename='dev.csv',\n",
        "      text_column='sentence',\n",
        "      label_column='label',\n",
        "      model_spec=mb_spec,\n",
        "      is_training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlHvVvv2hw4H"
      },
      "source": [
        "The Model Maker library also supports the `from_folder()` method to load data. It assumes that the text data of the same class are in the same subdirectory and that the subfolder name is the class name. Each text file contains one movie review sample. The `class_labels` parameter is used to specify which the subfolders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWuoensX4vDA"
      },
      "source": [
        "## Train a TensorFlow Model\n",
        "\n",
        "Train a text classification model using the training data.\n",
        "\n",
        "*Note: As MobileBERT is a complex model, each training epoch will takes about 10 minutes on a Colab GPU. Please make sure that you are using a GPU runtime.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvYSUuJY3QxR"
      },
      "outputs": [],
      "source": [
        "model = text_classifier.create(train_data, model_spec=mb_spec, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JKI-pNc8idH"
      },
      "source": [
        "Examine the detailed model structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd7Hs8TF8n3H"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP5FPk_tOxoZ"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "Evaluate the model that we have just trained using the test data and measure the loss and accuracy value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8c2ZQ0J3Riy"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esBGwHE2QxE8"
      },
      "source": [
        "## Export as a TensorFlow Lite model\n",
        "\n",
        "Convert the trained model to TensorFlow Lite model format with [metadata](https://www.tensorflow.org/lite/models/convert/metadata) so that you can later use in an on-device ML application. The label file and the vocab file are embedded in metadata. The default TFLite filename is `model.tflite`.\n",
        "\n",
        "In many on-device ML application, the model size is an important factor. Therefore, it is recommended that you apply quantize the model to make it smaller and potentially run faster.\n",
        "The default post-training quantization technique is dynamic range quantization for the BERT and MobileBERT models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im6wA9lK3TQB"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='mobilebert/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w12kvDdHJIGH"
      },
      "source": [
        "The TensorFlow Lite model file can be integrated in a mobile app using the [BertNLClassifier API](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_nl_classifier) in [TensorFlow Lite Task Library](https://www.tensorflow.org/lite/inference_with_metadata/task_library/overview). Please note that this is **different** from the `NLClassifier` API used to integrate the text classification trained with the average word vector model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVy0ormoMZwL"
      },
      "source": [
        "The export formats can be one or a list of the following:\n",
        "\n",
        "*   `ExportFormat.TFLITE`\n",
        "*   `ExportFormat.LABEL`\n",
        "*   `ExportFormat.VOCAB`\n",
        "*   `ExportFormat.SAVED_MODEL`\n",
        "\n",
        "By default, it exports only the TensorFlow Lite model file containing the model metadata. You can also choose to export other files related to the model for better examination. For instance, exporting only the label file and vocab file as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbK7nzK_Mfx4"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='mobilebert/', export_format=[ExportFormat.LABEL, ExportFormat.VOCAB])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZKYthlVrTos"
      },
      "source": [
        "You can evaluate the TFLite model with `evaluate_tflite` method to measure its accuracy. Converting the trained TensorFlow model to TFLite format and apply quantization can affect its accuracy so it is recommended to evaluate the TFLite model accuracy before deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ochbq95ZrVFX"
      },
      "outputs": [],
      "source": [
        "accuracy = model.evaluate_tflite('mobilebert/model.tflite', test_data)\n",
        "print('TFLite model accuracy: ', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoWiA_zX8rxE"
      },
      "source": [
        "## Advanced Usage\n",
        "\n",
        "The `create` function is the driver function that the Model Maker library uses to create models. The `model_spec` parameter defines the model specification. The `AverageWordVecSpec` and `BertClassifierSpec` classes are currently supported. The `create` function comprises of the following steps:\n",
        "\n",
        "1. Creates the model for the text classifier according to `model_spec`.\n",
        "2. Trains the classifier model.  The default epochs and the default batch size are set by the `default_training_epochs` and `default_batch_size` variables in the `model_spec` object.\n",
        "\n",
        "This section covers advanced usage topics like adjusting the model and the training hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8VxPiOLy4Gv"
      },
      "source": [
        "### Customize the MobileBERT model hyperparameters\n",
        "\n",
        "The model parameters you can adjust are:\n",
        "\n",
        "* `seq_len`: Length of the sequence to feed into the model.\n",
        "* `initializer_range`: The standard deviation of the `truncated_normal_initializer` for initializing all weight matrices.\n",
        "* `trainable`: Boolean that specifies whether the pre-trained layer is trainable.\n",
        "\n",
        "The training pipeline parameters you can adjust are:\n",
        "\n",
        "* `model_dir`: The location of the model checkpoint files. If not set, a temporary directory will be used.\n",
        "* `dropout_rate`: The dropout rate.\n",
        "* `learning_rate`: The initial learning rate for the Adam optimizer.\n",
        "* `tpu`: TPU address to connect to.\n",
        "\n",
        "For instance, you can set the `seq_len=256` (default is 128). This allows the model to classify longer text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tr9BLcjy4Sh"
      },
      "outputs": [],
      "source": [
        "new_model_spec = model_spec.get('mobilebert_classifier')\n",
        "new_model_spec.seq_len = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwtiksguDfhl"
      },
      "source": [
        "### Customize the average word embedding model hyperparameters\n",
        "\n",
        "You can adjust the model infrastructure like the `wordvec_dim` and the `seq_len` variables in the `AverageWordVecSpec` class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAOd5_bzH9AQ"
      },
      "source": [
        "For example, you can train the model with a larger value of `wordvec_dim`. Note that you must construct a new `model_spec` if you modify the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9WBN0UTQoMN"
      },
      "outputs": [],
      "source": [
        "new_model_spec = AverageWordVecSpec(wordvec_dim=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LSTdghTP0Cv"
      },
      "source": [
        "Get the preprocessed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVZurFBORG3J"
      },
      "outputs": [],
      "source": [
        "new_train_data = DataLoader.from_csv(\n",
        "      filename='train.csv',\n",
        "      text_column='sentence',\n",
        "      label_column='label',\n",
        "      model_spec=new_model_spec,\n",
        "      is_training=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD7QVVHeRZoM"
      },
      "source": [
        "Train the new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzpV246_JGEu"
      },
      "outputs": [],
      "source": [
        "model = text_classifier.create(new_train_data, model_spec=new_model_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQuy7RSDir3"
      },
      "source": [
        "### Tune the training hyperparameters\n",
        "You can also tune the training hyperparameters like `epochs` and `batch_size` that affect the model accuracy. For instance,\n",
        "\n",
        "*   `epochs`: more epochs could achieve better accuracy, but may lead to overfitting.\n",
        "*   `batch_size`: the number of samples to use in one training step.\n",
        "\n",
        "For example, you can train with more epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnWFaYZBG6NW"
      },
      "outputs": [],
      "source": [
        "model = text_classifier.create(new_train_data, model_spec=new_model_spec, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUaKQZBQHBQR"
      },
      "source": [
        "Evaluate the newly retrained model with 20 training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMPi1xflHDSY"
      },
      "outputs": [],
      "source": [
        "new_test_data = DataLoader.from_csv(\n",
        "      filename='dev.csv',\n",
        "      text_column='sentence',\n",
        "      label_column='label',\n",
        "      model_spec=new_model_spec,\n",
        "      is_training=False)\n",
        "\n",
        "loss, accuracy = model.evaluate(new_test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq6B9lKMfhS6"
      },
      "source": [
        "### Change the Model Architecture\n",
        "\n",
        "You can change the model by changing the `model_spec`. The following shows how to change to BERT-Base model.\n",
        "\n",
        "Change the `model_spec` to BERT-Base model for the text classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfFCWrwyggrT"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('bert_classifier')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2d7yycrgu6L"
      },
      "source": [
        "The remaining steps are the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgiD_tkyQn7l"
      },
      "source": [
        "### Customize Post-training quantization on the TensorFlow Lite model\n",
        "\n",
        "[Post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) is a conversion technique that can reduce model size and inference latency, while also improving CPU and hardware accelerator inference speed, with a little degradation in model accuracy. Thus, it's widely used to optimize the model.\n",
        "\n",
        "Model Maker library applies a default post-training quantization techique when exporting the model. If you want to customize post-training quantization, Model Maker supports multiple post-training quantization options using [QuantizationConfig](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/config/QuantizationConfig) as well. Let's take float16 quantization as an instance. First, define the quantization config.\n",
        "\n",
        "```python\n",
        "config = QuantizationConfig.for_float16()\n",
        "```\n",
        "\n",
        "\n",
        "Then we export the TensorFlow Lite model with such configuration.\n",
        "\n",
        "```python\n",
        "model.export(export_dir='.', tflite_filename='model_fp16.tflite', quantization_config=config)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkJGvMEx6VD-"
      },
      "source": [
        "# Read more\n",
        "\n",
        "You can read our [text classification](https://www.tensorflow.org/lite/examples/text_classification/overview) example to learn technical details. For more information, please refer to:\n",
        "\n",
        "*   TensorFlow Lite Model Maker [guide](https://www.tensorflow.org/lite/models/modify/model_maker) and [API reference](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker).\n",
        "*  Task Library: [NLClassifier](https://www.tensorflow.org/lite/inference_with_metadata/task_library/nl_classifier) and [BertNLClassifier](https://www.tensorflow.org/lite/inference_with_metadata/task_library/bert_nl_classifier) for deployment.\n",
        "*   The end-to-end reference apps: [Android](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification/android) and [iOS](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification/ios)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Model Maker Text Classification Tutorial",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
